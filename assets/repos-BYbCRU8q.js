import{j as e,R as r,P as l,O as d,y as o,C as c,z as u,X as f}from"./index-BV9s3UtC.js";function b({imageUrl:a,alt:i,open:n,onOpenChange:t}){return e.jsx(r,{open:n,onOpenChange:t,children:e.jsxs(l,{children:[e.jsx(d,{className:o("fixed inset-0 z-50 bg-black/80 backdrop-blur-sm","data-[state=open]:animate-in data-[state=closed]:animate-out","data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0")}),e.jsx(c,{className:o("fixed left-[50%] top-[50%] z-50 translate-x-[-50%] translate-y-[-50%]","data-[state=open]:animate-in data-[state=closed]:animate-out","data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0","data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95","data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%]","data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%]","w-full max-w-[95vw] max-h-[95vh] p-4"),onPointerDownOutside:s=>{s.preventDefault(),t(!1)},onEscapeKeyDown:()=>t(!1),children:e.jsxs("div",{className:"relative w-full h-full flex items-center justify-center",children:[e.jsx("img",{src:a,alt:i,className:"max-w-full max-h-[90vh] w-auto h-auto object-contain rounded-md"}),e.jsxs(u,{className:o("absolute top-2 right-2 rounded-sm opacity-70 ring-offset-background transition-opacity","hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2","disabled:pointer-events-none bg-background/80 backdrop-blur-sm p-2"),children:[e.jsx(f,{className:"h-4 w-4"}),e.jsx("span",{className:"sr-only",children:"Close"})]})]})})]})})}const m=[{title:"Sim2RealGen",role:"Stanford Vision and Learning Lab, Stanford University",duration:"Nov 2025 - Present",description:"Developing Sim2RealGen, a video-based sim-to-real learning  framework that enables robots to imagine and evaluate task-relevant scene  variations offline via generated RGBâ€“depth videos, improving real-world manipulation without physical trial-and-error.",tools:["Vision","Robotics","ML"],advisors:"Raven Huang, Prof. Jiajun Wu, Prof. Fei-Fei Li",github:null,paper:null,image:"images/sail.jpg",featured:!0},{title:"GLUESTICK (2026 ICML under review)",role:"Harvard Edge Computing Lab, Harvard University",duration:"Aug 2025 - Jan 2025",description:"Developed GLUSTICK, a post-pruning recovery method for VLA models that restores robotic safety and performance without retraining by combining model compression, SVD-based correction, and efficient deployment on edge hardware",tools:["VLA","Robotics","ML"],github:"https://gluestick-vla.github.io/",paper:"https://arxiv.org/abs/2510.08464",image:"images/gluestick.jpg",featured:!0},{title:"Vision-Guided Robotic Folding of Deformable Garments Using Model-Based Planning and Control",role:"MIT 6.4212 Final Project",duration:"Sep 2025 - Dec 2025",description:"Developed an interpretable square fabric folding system integrating geometric vision, deformable modeling, motion planning, and impedance control, achieving up to 90% success across varying fabric sizes.",tools:["Robotics","Vision","ML"],github:null,paper:"articles/Wang & Zhang 6.4212 Final Paper.pdf",image:"images/6.4212.jpg",featured:!0},{title:"FISOR + TELS",role:"AIR-DREAM Lab at the Institute for AI Industry Research (AIR) at Tsinghua University",duration:"May 2025 - Aug 2025",description:"Developed an offline RL model combining FISOR (Feasibility-guided Safe Offline RL)  and TELS (T-symmetry Enforced Latent State-Stitching) frameworks to enhance safe learning and optimize national data center cooling under limited, low-quality data using PyTorch.",tools:["Offline RL","ML"],github:null,image:"images/fisortels.png",featured:!0}],p=m,h=p.filter(a=>a.featured!==!1);export{b as I,h as f,p};
